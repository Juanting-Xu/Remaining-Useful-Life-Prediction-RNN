Start training for RULs:
global step:1 [epoch:0] [learning rate: 0.001000] train_loss:10320.584961
global step:201 [epoch:0] [learning rate: 0.001000] train_loss:2975.023438
global step:401 [epoch:1] [learning rate: 0.001000] train_loss:693.084839
global step:601 [epoch:1] [learning rate: 0.001000] train_loss:1142.469238
global step:801 [epoch:2] [learning rate: 0.001000] train_loss:2367.163574
global step:1001 [epoch:3] [learning rate: 0.001000] train_loss:755.756897
global step:1201 [epoch:3] [learning rate: 0.001000] train_loss:1374.172974
global step:1401 [epoch:4] [learning rate: 0.001000] train_loss:1144.450928
global step:1601 [epoch:5] [learning rate: 0.000990] train_loss:1114.986328
global step:1801 [epoch:5] [learning rate: 0.000990] train_loss:841.794678
global step:2001 [epoch:6] [learning rate: 0.000980] train_loss:583.505371
global step:2201 [epoch:7] [learning rate: 0.000970] train_loss:726.345825
global step:2401 [epoch:7] [learning rate: 0.000970] train_loss:1592.963379
global step:2601 [epoch:8] [learning rate: 0.000961] train_loss:966.136597
global step:2801 [epoch:9] [learning rate: 0.000951] train_loss:283.556458
global step:3001 [epoch:9] [learning rate: 0.000951] train_loss:1213.488281
global step:3201 [epoch:10] [learning rate: 0.000941] train_loss:525.714233
global step:3401 [epoch:11] [learning rate: 0.000932] train_loss:500.328491
global step:3601 [epoch:11] [learning rate: 0.000932] train_loss:194.867172
global step:3801 [epoch:12] [learning rate: 0.000923] train_loss:1295.061768
global step:4001 [epoch:13] [learning rate: 0.000914] train_loss:373.626740
global step:4201 [epoch:13] [learning rate: 0.000914] train_loss:289.717621
global step:4401 [epoch:14] [learning rate: 0.000904] train_loss:234.423950
global step:4601 [epoch:14] [learning rate: 0.000904] train_loss:1348.869019
global step:4801 [epoch:15] [learning rate: 0.000895] train_loss:169.871323
global step:5001 [epoch:16] [learning rate: 0.000886] train_loss:760.410950
global step:5201 [epoch:16] [learning rate: 0.000886] train_loss:270.567413
global step:5401 [epoch:17] [learning rate: 0.000878] train_loss:667.872253
global step:5601 [epoch:18] [learning rate: 0.000869] train_loss:199.570816
global step:5801 [epoch:18] [learning rate: 0.000869] train_loss:274.907715
global step:6001 [epoch:19] [learning rate: 0.000860] train_loss:303.061432
global step:6201 [epoch:20] [learning rate: 0.000851] train_loss:349.176025
global step:6401 [epoch:20] [learning rate: 0.000851] train_loss:465.486084
global step:6601 [epoch:21] [learning rate: 0.000843] train_loss:177.609879
global step:6801 [epoch:22] [learning rate: 0.000835] train_loss:547.889893
global step:7001 [epoch:22] [learning rate: 0.000835] train_loss:284.926117
global step:7201 [epoch:23] [learning rate: 0.000826] train_loss:109.240288
global step:7401 [epoch:24] [learning rate: 0.000818] train_loss:526.621155
global step:7601 [epoch:24] [learning rate: 0.000818] train_loss:486.357666
global step:7801 [epoch:25] [learning rate: 0.000810] train_loss:267.031708
global step:8001 [epoch:26] [learning rate: 0.000802] train_loss:148.464569
global step:8201 [epoch:26] [learning rate: 0.000802] train_loss:188.591995
global step:8401 [epoch:27] [learning rate: 0.000794] train_loss:842.461243
global step:8601 [epoch:28] [learning rate: 0.000786] train_loss:330.814423
global step:8801 [epoch:28] [learning rate: 0.000786] train_loss:406.932678
global step:9001 [epoch:29] [learning rate: 0.000778] train_loss:345.308533
global step:9201 [epoch:29] [learning rate: 0.000778] train_loss:960.319153
global step:9401 [epoch:30] [learning rate: 0.000770] train_loss:75.302704
global step:9601 [epoch:31] [learning rate: 0.000762] train_loss:57.139996
global step:9801 [epoch:31] [learning rate: 0.000762] train_loss:225.472839
global step:10001 [epoch:32] [learning rate: 0.000755] train_loss:789.834290
global step:10201 [epoch:33] [learning rate: 0.000747] train_loss:64.460373
global step:10401 [epoch:33] [learning rate: 0.000747] train_loss:45.763725
global step:10601 [epoch:34] [learning rate: 0.000740] train_loss:641.506470
global step:10801 [epoch:35] [learning rate: 0.000732] train_loss:1396.726318
global step:11001 [epoch:35] [learning rate: 0.000732] train_loss:83.688965
global step:11201 [epoch:36] [learning rate: 0.000725] train_loss:417.196533
global step:11401 [epoch:37] [learning rate: 0.000718] train_loss:268.405884
global step:11601 [epoch:37] [learning rate: 0.000718] train_loss:1091.957642
global step:11801 [epoch:38] [learning rate: 0.000711] train_loss:537.280090
global step:12001 [epoch:39] [learning rate: 0.000703] train_loss:248.519394
global step:12201 [epoch:39] [learning rate: 0.000703] train_loss:9.819238
global step:12401 [epoch:40] [learning rate: 0.000696] train_loss:1277.561890
global step:12601 [epoch:41] [learning rate: 0.000689] train_loss:50.037460
global step:12801 [epoch:41] [learning rate: 0.000689] train_loss:491.224640
global step:13001 [epoch:42] [learning rate: 0.000683] train_loss:800.335083
global step:13201 [epoch:42] [learning rate: 0.000683] train_loss:138.016113
global step:13401 [epoch:43] [learning rate: 0.000676] train_loss:120.413734
global step:13601 [epoch:44] [learning rate: 0.000669] train_loss:1812.986450
global step:13801 [epoch:44] [learning rate: 0.000669] train_loss:18.272182
global step:14001 [epoch:45] [learning rate: 0.000662] train_loss:1283.704956
global step:14201 [epoch:46] [learning rate: 0.000656] train_loss:149.756943
global step:14401 [epoch:46] [learning rate: 0.000656] train_loss:653.401367
global step:14601 [epoch:47] [learning rate: 0.000649] train_loss:109.841156
global step:14801 [epoch:48] [learning rate: 0.000643] train_loss:2224.584229
global step:15001 [epoch:48] [learning rate: 0.000643] train_loss:293.523285
global step:15201 [epoch:49] [learning rate: 0.000636] train_loss:6.103195