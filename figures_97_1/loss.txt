
Start training for RULs:
global step:1 [epoch:0] [learning rate: 0.001000] train_loss:10150234.000000
global step:201 [epoch:0] [learning rate: 0.001000] train_loss:30118.230469
global step:401 [epoch:1] [learning rate: 0.001000] train_loss:5727.066895
global step:601 [epoch:1] [learning rate: 0.001000] train_loss:4625.660156
global step:801 [epoch:2] [learning rate: 0.001000] train_loss:2136.563477
global step:1001 [epoch:3] [learning rate: 0.001000] train_loss:17111.347656
global step:1201 [epoch:3] [learning rate: 0.001000] train_loss:152799.421875
global step:1401 [epoch:4] [learning rate: 0.001000] train_loss:14900.684570
global step:1601 [epoch:5] [learning rate: 0.000990] train_loss:2962.670654
global step:1801 [epoch:5] [learning rate: 0.000990] train_loss:2972.588379
global step:2001 [epoch:6] [learning rate: 0.000980] train_loss:16073.802734
global step:2201 [epoch:7] [learning rate: 0.000970] train_loss:1856.405884
global step:2401 [epoch:7] [learning rate: 0.000970] train_loss:1334.444946
global step:2601 [epoch:8] [learning rate: 0.000961] train_loss:2665.677734
global step:2801 [epoch:9] [learning rate: 0.000951] train_loss:1437.300049
global step:3001 [epoch:9] [learning rate: 0.000951] train_loss:1773.578369
global step:3201 [epoch:10] [learning rate: 0.000941] train_loss:2338.914551
global step:3401 [epoch:11] [learning rate: 0.000932] train_loss:1840.156494
global step:3601 [epoch:11] [learning rate: 0.000932] train_loss:1758.252563
global step:3801 [epoch:12] [learning rate: 0.000923] train_loss:1810.368164
global step:4001 [epoch:13] [learning rate: 0.000914] train_loss:2781.751953
global step:4201 [epoch:13] [learning rate: 0.000914] train_loss:1776.387939
global step:4401 [epoch:14] [learning rate: 0.000904] train_loss:4099.195312
global step:4601 [epoch:14] [learning rate: 0.000904] train_loss:828.236267
global step:4801 [epoch:15] [learning rate: 0.000895] train_loss:8750.930664
global step:5001 [epoch:16] [learning rate: 0.000886] train_loss:1209.671753
global step:5201 [epoch:16] [learning rate: 0.000886] train_loss:2392.632080
global step:5401 [epoch:17] [learning rate: 0.000878] train_loss:6543.358887
global step:5601 [epoch:18] [learning rate: 0.000869] train_loss:2374.427979
global step:5801 [epoch:18] [learning rate: 0.000869] train_loss:697.901611
global step:6001 [epoch:19] [learning rate: 0.000860] train_loss:852.188782
global step:6201 [epoch:20] [learning rate: 0.000851] train_loss:1208.301514
global step:6401 [epoch:20] [learning rate: 0.000851] train_loss:567.416565
global step:6601 [epoch:21] [learning rate: 0.000843] train_loss:1291.661377
global step:6801 [epoch:22] [learning rate: 0.000835] train_loss:3112.879150
global step:7001 [epoch:22] [learning rate: 0.000835] train_loss:5369.114258
global step:7201 [epoch:23] [learning rate: 0.000826] train_loss:1283.656982
global step:7401 [epoch:24] [learning rate: 0.000818] train_loss:2469.289795
global step:7601 [epoch:24] [learning rate: 0.000818] train_loss:2307.447754
global step:7801 [epoch:25] [learning rate: 0.000810] train_loss:12219.884766
global step:8001 [epoch:26] [learning rate: 0.000802] train_loss:4315.625488
global step:8201 [epoch:26] [learning rate: 0.000802] train_loss:865.722290
global step:8401 [epoch:27] [learning rate: 0.000794] train_loss:954.385437
global step:8601 [epoch:28] [learning rate: 0.000786] train_loss:942.823486
global step:8801 [epoch:28] [learning rate: 0.000786] train_loss:225.758560
global step:9001 [epoch:29] [learning rate: 0.000778] train_loss:1161.059082
global step:9201 [epoch:29] [learning rate: 0.000778] train_loss:4380.008301
global step:9401 [epoch:30] [learning rate: 0.000770] train_loss:2024.620605
global step:9601 [epoch:31] [learning rate: 0.000762] train_loss:2995.965820
global step:9801 [epoch:31] [learning rate: 0.000762] train_loss:2835.595703
global step:10001 [epoch:32] [learning rate: 0.000755] train_loss:444.940765
global step:10201 [epoch:33] [learning rate: 0.000747] train_loss:16173.296875
global step:10401 [epoch:33] [learning rate: 0.000747] train_loss:11773.674805
global step:10601 [epoch:34] [learning rate: 0.000740] train_loss:1073.072510
global step:10801 [epoch:35] [learning rate: 0.000732] train_loss:1678.737915
global step:11001 [epoch:35] [learning rate: 0.000732] train_loss:247.815048
global step:11201 [epoch:36] [learning rate: 0.000725] train_loss:29354.912109
global step:11401 [epoch:37] [learning rate: 0.000718] train_loss:3347.798340
global step:11601 [epoch:37] [learning rate: 0.000718] train_loss:74.524048
global step:11801 [epoch:38] [learning rate: 0.000711] train_loss:1210.160034
global step:12001 [epoch:39] [learning rate: 0.000703] train_loss:1160.265991
global step:12201 [epoch:39] [learning rate: 0.000703] train_loss:10285.200195
global step:12401 [epoch:40] [learning rate: 0.000696] train_loss:148.580032
global step:12601 [epoch:41] [learning rate: 0.000689] train_loss:23586.781250
global step:12801 [epoch:41] [learning rate: 0.000689] train_loss:78.474121
global step:13001 [epoch:42] [learning rate: 0.000683] train_loss:1414.412598
global step:13201 [epoch:42] [learning rate: 0.000683] train_loss:1849.513672
global step:13401 [epoch:43] [learning rate: 0.000676] train_loss:10605.131836
global step:13601 [epoch:44] [learning rate: 0.000669] train_loss:1558.628052
global step:13801 [epoch:44] [learning rate: 0.000669] train_loss:1657.079468
global step:14001 [epoch:45] [learning rate: 0.000662] train_loss:16402.593750
global step:14201 [epoch:46] [learning rate: 0.000656] train_loss:207.368896
global step:14401 [epoch:46] [learning rate: 0.000656] train_loss:200.033051
global step:14601 [epoch:47] [learning rate: 0.000649] train_loss:6118.885742
global step:14801 [epoch:48] [learning rate: 0.000643] train_loss:80.618935
global step:15001 [epoch:48] [learning rate: 0.000643] train_loss:40897.945312
global step:15201 [epoch:49] [learning rate: 0.000636] train_loss:318.448975
