
Start training for RULs:
global step:1 [epoch:0] [learning rate: 0.001000] train_loss:7640.500977
global step:201 [epoch:0] [learning rate: 0.001000] train_loss:905.830261
global step:401 [epoch:1] [learning rate: 0.001000] train_loss:352.555603
global step:601 [epoch:1] [learning rate: 0.001000] train_loss:307.549805
global step:801 [epoch:2] [learning rate: 0.001000] train_loss:1335.991943
global step:1001 [epoch:3] [learning rate: 0.001000] train_loss:685.682495
global step:1201 [epoch:3] [learning rate: 0.001000] train_loss:326.797546
global step:1401 [epoch:4] [learning rate: 0.001000] train_loss:555.928833
global step:1601 [epoch:5] [learning rate: 0.000990] train_loss:504.150635
global step:1801 [epoch:5] [learning rate: 0.000990] train_loss:551.741211
global step:2001 [epoch:6] [learning rate: 0.000980] train_loss:147.718750
global step:2201 [epoch:7] [learning rate: 0.000970] train_loss:109.720512
global step:2401 [epoch:7] [learning rate: 0.000970] train_loss:60.431866
global step:2601 [epoch:8] [learning rate: 0.000961] train_loss:149.572556
global step:2801 [epoch:9] [learning rate: 0.000951] train_loss:392.899109
global step:3001 [epoch:9] [learning rate: 0.000951] train_loss:460.006866
global step:3201 [epoch:10] [learning rate: 0.000941] train_loss:435.299500
global step:3401 [epoch:11] [learning rate: 0.000932] train_loss:256.273254
global step:3601 [epoch:11] [learning rate: 0.000932] train_loss:311.644745
global step:3801 [epoch:12] [learning rate: 0.000923] train_loss:141.556763
global step:4001 [epoch:13] [learning rate: 0.000914] train_loss:97.741913
global step:4201 [epoch:13] [learning rate: 0.000914] train_loss:39.298725
global step:4401 [epoch:14] [learning rate: 0.000904] train_loss:81.179626
global step:4601 [epoch:14] [learning rate: 0.000904] train_loss:48.088467
global step:4801 [epoch:15] [learning rate: 0.000895] train_loss:189.558655
global step:5001 [epoch:16] [learning rate: 0.000886] train_loss:144.889877
global step:5201 [epoch:16] [learning rate: 0.000886] train_loss:593.339417
global step:5401 [epoch:17] [learning rate: 0.000878] train_loss:64.008255
global step:5601 [epoch:18] [learning rate: 0.000869] train_loss:361.103088
global step:5801 [epoch:18] [learning rate: 0.000869] train_loss:463.166931
global step:6001 [epoch:19] [learning rate: 0.000860] train_loss:273.901001
global step:6201 [epoch:20] [learning rate: 0.000851] train_loss:23.144495
global step:6401 [epoch:20] [learning rate: 0.000851] train_loss:185.109680
global step:6601 [epoch:21] [learning rate: 0.000843] train_loss:255.922531
global step:6801 [epoch:22] [learning rate: 0.000835] train_loss:702.493896
global step:7001 [epoch:22] [learning rate: 0.000835] train_loss:187.233444
global step:7201 [epoch:23] [learning rate: 0.000826] train_loss:851.110962
global step:7401 [epoch:24] [learning rate: 0.000818] train_loss:196.372910
global step:7601 [epoch:24] [learning rate: 0.000818] train_loss:240.436615
global step:7801 [epoch:25] [learning rate: 0.000810] train_loss:27.670498
global step:8001 [epoch:26] [learning rate: 0.000802] train_loss:50.289700
global step:8201 [epoch:26] [learning rate: 0.000802] train_loss:228.038391
global step:8401 [epoch:27] [learning rate: 0.000794] train_loss:73.612152
global step:8601 [epoch:28] [learning rate: 0.000786] train_loss:44.553898
global step:8801 [epoch:28] [learning rate: 0.000786] train_loss:16.376219
global step:9001 [epoch:29] [learning rate: 0.000778] train_loss:88.575035
global step:9201 [epoch:29] [learning rate: 0.000778] train_loss:72.112244
global step:9401 [epoch:30] [learning rate: 0.000770] train_loss:257.340515
global step:9601 [epoch:31] [learning rate: 0.000762] train_loss:723.463928
global step:9801 [epoch:31] [learning rate: 0.000762] train_loss:25.224113
global step:10001 [epoch:32] [learning rate: 0.000755] train_loss:26.796873
global step:10201 [epoch:33] [learning rate: 0.000747] train_loss:444.379120
global step:10401 [epoch:33] [learning rate: 0.000747] train_loss:80.183701
global step:10601 [epoch:34] [learning rate: 0.000740] train_loss:250.139038
global step:10801 [epoch:35] [learning rate: 0.000732] train_loss:47.386047
global step:11001 [epoch:35] [learning rate: 0.000732] train_loss:268.376617
global step:11201 [epoch:36] [learning rate: 0.000725] train_loss:347.165741
global step:11401 [epoch:37] [learning rate: 0.000718] train_loss:10.169038
global step:11601 [epoch:37] [learning rate: 0.000718] train_loss:1070.703979
global step:11801 [epoch:38] [learning rate: 0.000711] train_loss:548.416504
global step:12001 [epoch:39] [learning rate: 0.000703] train_loss:3.737745
global step:12201 [epoch:39] [learning rate: 0.000703] train_loss:146.716217
global step:12401 [epoch:40] [learning rate: 0.000696] train_loss:21.103712
global step:12601 [epoch:41] [learning rate: 0.000689] train_loss:101.736885
global step:12801 [epoch:41] [learning rate: 0.000689] train_loss:1133.320190
global step:13001 [epoch:42] [learning rate: 0.000683] train_loss:22.337196
global step:13201 [epoch:42] [learning rate: 0.000683] train_loss:82.261230
global step:13401 [epoch:43] [learning rate: 0.000676] train_loss:8.227377
global step:13601 [epoch:44] [learning rate: 0.000669] train_loss:34.379936
global step:13801 [epoch:44] [learning rate: 0.000669] train_loss:11.239144
global step:14001 [epoch:45] [learning rate: 0.000662] train_loss:9.307308
global step:14201 [epoch:46] [learning rate: 0.000656] train_loss:442.874146
global step:14401 [epoch:46] [learning rate: 0.000656] train_loss:43.242390
global step:14601 [epoch:47] [learning rate: 0.000649] train_loss:79.391541
global step:14801 [epoch:48] [learning rate: 0.000643] train_loss:389.349854
global step:15001 [epoch:48] [learning rate: 0.000643] train_loss:9.001774
global step:15201 [epoch:49] [learning rate: 0.000636] train_loss:591.471130
